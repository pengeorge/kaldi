#!/bin/bash

# Copyright 2012  Johns Hopkins University (Author: Guoguo Chen)
# Apache 2.0.

nj=12
cmd=run.pl
beam=5                    # Beam for proxy FST; usually used together with the nbest option
nbest=100                 # First n best proxy keywords
phone_cutoff=5            # We don't generate proxy keywords for OOV keywords that have less phones
                          # than the specified cutoff; they may introduce more false alarms
count_cutoff=1            # Cutoff for the phone confusion pair counts 
confusion_matrix=

[ -f ./path.sh ] && . ./path.sh; # source the path.
. parse_options.sh || exit 1;

if [ $# -ne 4 ]; then
  echo "Generate proxy keywords for OOV keywords. You may apply the confusion matrix. If you"
  echo "are going to use the confusion matrix, please use the following format for the file"
  echo "\$confusion_matrix:"
  echo "          p1 p2 count1        // For substitution"
  echo "          p3 <eps> count2     // For deletion"
  echo "          <eps> p4 count3     // For insertion"
  echo ""
  echo "Usage: local/generate_example_kws.sh <kws-data-dir> <oov-lexicon>"
  echo "                                     <lexicon> <symbol-table>"
  echo " e.g.: local/generate_example_kws.sh data/kws oov_lexicon.txt"
  echo "                                     data/local/lang/align_lexicon.txt data/lang/words.txt"
  exit 1;
fi

# Parameters
kwsdatadir=$1
oov_lexicon=$2
original_lexicon=$3
original_symtab=$4

mkdir -p $kwsdatadir/tmp
cp $original_lexicon $kwsdatadir/tmp/original.lex

# You may modify the lexicon here; For example, I removed the stress marks for the 
# Tagalog lexicon
#cat $oov_lexicon |\
#  sed 's/_[%|"]//g' | awk '{if(NF>=2) {print $0}}' > $kwsdatadir/tmp/oov.lex
#cat $original_lexicon |\
#  sed 's/_[%|"]//g' | awk '{if(NF>=2) {print $0}}' > $kwsdatadir/tmp/original.lex

# Get OOV keywords, and remove the short OOV keywords. Generate proxy keywords based
# on the phone confusion for the short OOV keywords may introduce a lot of false alarms,
# therefore we provide the cutoff option.
cat $kwsdatadir/kwlist_outvocab.xml | \
  grep -o -P "(?<=kwid=\").*(?=\")" |\
  paste - <(cat $kwsdatadir/kwlist_outvocab.xml | grep -o -P "(?<=<kwtext>).*(?=</kwtext>)") \
  > $kwsdatadir/tmp/oov_all.txt
cat $kwsdatadir/tmp/oov_all.txt | perl -e '
  require "czpScripts/utils/libCase.pl";
  open(W, "<'$kwsdatadir/oov.lex'") || die "Fail to open OOV lexicon: '$kwsdatadir/oov.lex'\n";
  my %lexicon;
  while (<W>) {
    chomp;
    my @col = split();
    @col >= 2 || die "Bad line in lexicon: $_\n";
    $lexicon{$col[0]} = scalar(@col)-1;
  }
  while (<>) {
    chomp;
    my @col = split();
    @col >= 2 || die "Bad line in keywords file: $_\n";
    my $key = shift @col;
    my $query = &tolower(join(" ", @col));
    @col = split(/\s+/, $query);
    my $len = 0;
    for (my $i = 0; $i < scalar(@col); $i ++) {
      my $word = $col[$i];
      if (defined($lexicon{$word})) {
        $len += $lexicon{$word};
      } else {
        print STEDRR "No pronunciation found for word: $col[$i]\n";
      }
    }
    if ($len >= '$phone_cutoff') {
      print "$key\t$query\n";
    }
  }' > $kwsdatadir/tmp/oov.txt

# Get phone symbols
cat $kwsdatadir/oov.lex $kwsdatadir/tmp/original.lex |\
  awk '{for(i=2; i <= NF; i++) {print $i;}}' | sort -u |\
  sed '1i\<eps>' | awk 'BEGIN{x=0} {print $0"\t"x; x++;}' > $kwsdatadir/tmp/phones.txt

# Get word symbols; We append new words to the original word symbol table (Jan 6, 2014, chenzp edit: trans oov.txt to lower case)
max_id=`cat $original_symtab | awk '{print $2}' | sort -n | tail -1`;
cat $kwsdatadir/tmp/oov.txt |\
  awk '{for(i=2; i <= NF; i++) {print $i;}}' |\
  uconv -f utf8 -t utf8 -x 'Any-Lower' |\
  cat - <(cat $kwsdatadir/oov.lex | awk '{print $1;}') |\
  cat - <(cat $kwsdatadir/tmp/original.lex | awk '{print $1}') | sort -u |\
  grep -F -v -x -f <(cat $original_symtab | awk '{print $1;}') |\
  awk 'BEGIN{x='$max_id'+1}{print $0"\t"x; x++;}' |\
  cat $original_symtab - > $kwsdatadir/tmp/words.txt

# Compile lexicon into FST
# if L2 (oov_lexicon.fst) includes both oov.lex & original_lexicon.fst, 
# the proxy network will become too huge to composite.
#cat $kwsdatadir/oov.lex $kwsdatadir/tmp/original.lex | utils/make_lexicon_fst.pl - |\
cat $kwsdatadir/oov.lex | utils/make_lexicon_fst.pl - |\
  fstcompile --isymbols=$kwsdatadir/tmp/phones.txt --osymbols=$kwsdatadir/tmp/words.txt - |\
  fstinvert | fstarcsort --sort_type=olabel > $kwsdatadir/tmp/oov_lexicon.fst
cat $kwsdatadir/tmp/original.lex | utils/make_lexicon_fst.pl - |\
  fstcompile --isymbols=$kwsdatadir/tmp/phones.txt --osymbols=$kwsdatadir/tmp/words.txt - |\
  fstarcsort --sort_type=ilabel > $kwsdatadir/tmp/original_lexicon.fst

# Compile E.fst
if [ -z $confusion_matrix ]; then
  cat $kwsdatadir/tmp/phones.txt |\
    grep -v -E "<.*>" | grep -v "sil" | awk '{print $1;}' |\
    czpScripts/kws/build_edit_distance_fst.chenzp.pl --boundary-off=false - - |\
    fstcompile --isymbols=$kwsdatadir/tmp/phones.txt --osymbols=$kwsdatadir/tmp/phones.txt - $kwsdatadir/tmp/Edit.fst
else
  echo "$0: Using confusion matrix."
  czpScripts/kws/count_to_logprob.chenzp.pl --cutoff $count_cutoff $confusion_matrix $kwsdatadir/tmp/confusion.txt
  cat $kwsdatadir/tmp/phones.txt |\
    grep -v -E "<.*>" | grep -v "sil" | awk '{print $1;}' |\
    czpScripts/kws/build_edit_distance_fst.chenzp.pl --boundary-off=false \
    --confusion-matrix=$kwsdatadir/tmp/confusion.txt - - |\
    fstcompile --isymbols=$kwsdatadir/tmp/phones.txt --osymbols=$kwsdatadir/tmp/phones.txt - $kwsdatadir/tmp/Edit.fst
fi

# Pre-compose L2 and E, for the sake of efficiency
fstcompose $kwsdatadir/tmp/oov_lexicon.fst $kwsdatadir/tmp/Edit.fst |\
  fstarcsort --sort_type=olabel > $kwsdatadir/tmp/L2xE.fst

# Prepare for parallelization
cat $kwsdatadir/tmp/oov.txt | utils/sym2int.pl -f 2- $kwsdatadir/tmp/words.txt > $kwsdatadir/tmp/oov.int
cat $kwsdatadir/tmp/oov.int | grep -P '^\S+ \d+$' > $kwsdatadir/tmp/oov_1gram.int
#cat $kwsdatadir/tmp/oov.int | grep -P '^\S+ \d+ \d+$' > $kwsdatadir/tmp/oov_2gram.int
#cat $kwsdatadir/tmp/oov.int | grep -P '^\S+ \d+ \d+ \d+$' > $kwsdatadir/tmp/oov_3gram.int
cat $kwsdatadir/tmp/oov.int | grep -P '^\S+ \d+( \d+)+$' > $kwsdatadir/tmp/oov_mgram.int

# find the OOVs which are not single query but a part of multi-word query
cat $kwsdatadir/tmp/oov_mgram.int | perl -e '
  my %isoov;
  while (<STDIN>) {
      chomp;
      @col = split();
      shift @col;
      for (@col) {
          if ($_ > '$max_id') {
              $isoov{$_} = 1;
          }
      }
  }
  open(EXC, "'$kwsdatadir'/tmp/oov_1gram.int") || die "Cannot open file oov_1gram.int\n";
  while (<EXC>) {
      chomp;
      @col = split();
      if ($isoov{$col[1]}) {
          undef $isoov{$col[1]};
      }
  }
  close(EXC);
  my $k = 1;
  for (sort keys %isoov) {
      printf "KWTMP-%04d %d\n", $k, $_;
      $k++;
  }' > $kwsdatadir/tmp/oov_part_of_query.int

# only generate proxy keywords for single OOV (chenzp)
nj0=$nj
for suf in _{1gram,part_of_query}; do
#false &&
{
  nj=$nj0
  if [ $nj -gt `cat $kwsdatadir/tmp/oov$suf.int | wc -l` ]; then
    nj=`cat $kwsdatadir/tmp/oov$suf.int | wc -l`
    echo "$0: Too many number of jobs, using $nj instead"
  fi
  mkdir -p $kwsdatadir/tmp/split$suf/
  for j in `seq 1 $nj`; do
    let "id=$j-1";
    utils/split_scp.pl -j $nj $id $kwsdatadir/tmp/oov$suf.int $kwsdatadir/tmp/split$suf/$j.int
  done
  
  # Generate the proxy keywords
  $cmd JOB=1:$nj $kwsdatadir/tmp/split$suf/JOB.log \
    generate-proxy-keywords --verbose=1 \
    --cost-threshold=$beam --nBest=$nbest \
    $kwsdatadir/tmp/L2xE.fst $kwsdatadir/tmp/original_lexicon.fst \
    ark:$kwsdatadir/tmp/split$suf/JOB.int ark:$kwsdatadir/tmp/split$suf/JOB.fsts
  cat $kwsdatadir/tmp/split$suf/*.fsts > $kwsdatadir/keywords_outvocab$suf.fsts
}
done

cat $kwsdatadir/keywords_outvocab_{1gram,part_of_query}.fsts > $kwsdatadir/oov_proxies_all1gram.fsts

# simply substitute oov with proxy word in multiword query (chenzp)
fsts-to-transcripts ark:$kwsdatadir/oov_proxies_all1gram.fsts ark,t:$kwsdatadir/oov_proxies_all1gram.trans
for suf in _mgram; do
  cat $kwsdatadir/tmp/oov$suf.int | perl -e '
    open(OOV, "'$kwsdatadir'/tmp/oov_1gram.int") or die "cannot open oov_1gram.int\n";
    my %oovid;
    while (<OOV>) {
        chomp;
        my @col = split();
        $oovid{$col[0]} = $col[1];
    }
    close(OOV);
    open(OOV, "'$kwsdatadir'/tmp/oov_part_of_query.int") or die "cannot open oov_part_of_query.int\n";
    my %oovid;
    while (<OOV>) {
        chomp;
        my @col = split();
        $oovid{$col[0]} = $col[1];
    }
    close(OOV);
    open(PROXY, "'$kwsdatadir'/oov_proxies_all1gram.trans") or die "cannot open oov_proxies_all1gram.trans\n";
    my %proxy;
    while (<PROXY>) {
        chomp;
        my @col = split();
        my $kwid = shift @col;
        push(@{$proxy{$oovid{$kwid}}}, @col);
    }
    close(PROXY);

    while (<>) {
        chomp;
        my @col = split();
        print (shift @col);
        while ($id = shift @col) {
            if (defined($proxy{$id})) {
                print " ".join(" ", @{$proxy{$id}});
            } else {
                print " $id";
            }
        }
        print "\n";
    }' > $kwsdatadir/tmp/oov_proxy$suf.int
cat $kwsdatadir/tmp/oov_proxy$suf.int | transcripts-to-fsts ark,t:- ark,t:$kwsdatadir/keywords_outvocab$suf.fsts
done

# Post process
if [ ! -f $kwsdatadir/keywords_invocab.fsts ]; then
  cp -f $kwsdatadir/keywords.fsts $kwsdatadir/keywords_invocab.fsts
fi
#cat $kwsdatadir/tmp/split/*.fsts > $kwsdatadir/keywords_outvocab.fsts
#cat $kwsdatadir/keywords_outvocab_{2,3}gram.fsts > $kwsdatadir/keywords_outvocab_mgram.fsts
cat $kwsdatadir/keywords_outvocab_{1,m}gram.fsts > $kwsdatadir/keywords_outvocab.fsts
cat $kwsdatadir/keywords_invocab.fsts $kwsdatadir/keywords_outvocab.fsts \
  > $kwsdatadir/keywords.fsts
